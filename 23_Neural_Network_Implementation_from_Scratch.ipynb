{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Network Implementation from scratch using \"pima indians-diabetes-database\"**"
      ],
      "metadata": {
        "id": "GboQ_t5JFFYW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtI_20Ph6l54",
        "outputId": "2154f21a-aed1-48ee-ea26-01d4effa8f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "# Install Kaggle API\n",
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "nGoZ_Eir92tl",
        "outputId": "4b2713d3-242e-4102-8cd1-706767fb9949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06475516-7d62-4ce4-9e73-c1986b86ed91\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06475516-7d62-4ce4-9e73-c1986b86ed91\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mdyalam\",\"key\":\"c2e739ee73a646cad89e903c817a0649\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move kaggle.json to the Correct Directory\n",
        "import os\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "GC7jInW49_Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all dataset in Kaggle\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hmx8voN-Jrc",
        "outputId": "39304256-d109-4ac8-89af-2c00e9d23ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                              title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "asinow/car-price-dataset                                         Car Price Dataset                                135KB  2025-01-26 19:53:28          20468        329  1.0              \n",
            "abdulmalik1518/mobiles-dataset-2025                              Mobiles Dataset (2025)                            20KB  2025-02-18 06:50:24           4084         79  1.0              \n",
            "mahmoudelhemaly/students-grading-dataset                         Student Performance & Behavior Dataset           508KB  2025-02-17 17:38:46           3587         69  1.0              \n",
            "adilshamim8/workout-and-fitness-tracker-data                     Workout & Fitness Tracker Dataset                272KB  2025-02-08 06:11:18           1783         30  1.0              \n",
            "asinow/diabetes-dataset                                          Diabetes Dataset                                 224KB  2025-02-20 08:38:56           2372         36  1.0              \n",
            "vinothkannaece/sales-dataset                                     sales dataset                                     27KB  2025-02-18 05:13:42           3839         57  1.0              \n",
            "artyomkruglov/gaming-profiles-2025-steam-playstation-xbox        Gaming Profiles 2025 (Steam, PlayStation, Xbox)  889MB  2025-02-25 10:37:51            647         23  1.0              \n",
            "himelsarder/coffee-shop-daily-revenue-prediction-dataset         Coffee Shop Daily Revenue Prediction Dataset      30KB  2025-02-07 07:29:16           2235         28  1.0              \n",
            "samithsachidanandan/air-traffic-in-europe-from-2016-to-2024      Air Traffic in Europe from 2016 to 2024            9MB  2025-02-09 04:46:18           1073         27  1.0              \n",
            "bhargavchirumamilla/thyroid-cancer-risk-dataset                  Thyroid Cancer Risk Dataset                        4MB  2025-02-04 03:55:56           1143         29  1.0              \n",
            "samithsachidanandan/world-happiness-report-2020-2024             World Happiness Report (2020-2024)                22KB  2025-02-19 16:39:47            927         25  1.0              \n",
            "adilshamim8/economic-indicators-and-inflation                    Economic Indicators & Inflation                    7KB  2025-02-14 08:49:30           1034         25  1.0              \n",
            "samikshadalvi/pcos-diagnosis-dataset                             PCOS Diagnosis Dataset                             7KB  2025-02-19 23:03:44            634         24  1.0              \n",
            "sahityasetu/ufo-sightings                                        UFO Sightings                                      5MB  2025-02-15 08:47:02            651         23  1.0              \n",
            "adilshamim8/student-performance-and-learning-style               Student Performance & Learning Style             148KB  2025-02-12 06:12:07           1674         33  1.0              \n",
            "rishabhpancholi1302/spotify-most-popular-songs-dataset           Spotify Most Popular Songs Dataset                 4MB  2025-02-21 10:45:03           1125         24  0.8235294        \n",
            "adilshamim8/education-and-career-success                         Education & Career Success.                      118KB  2025-02-03 05:24:20           5358         75  1.0              \n",
            "samayashar/fraud-detection-transactions-dataset                  Fraud Detection Transactions Dataset               2MB  2025-02-21 18:06:23           1511         33  1.0              \n",
            "shankarpriya2913/crop-and-soil-dataset                           Crop and Soil DataSet                            107KB  2025-01-28 08:32:23           1672         28  1.0              \n",
            "ankushpanday2/heart-attack-risk-and-prediction-dataset-in-india  Heart Attack Risk & Prediction Dataset In India  269KB  2025-02-25 16:53:26           1040         25  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Pima Indians Diabetes Dataset\n",
        "!kaggle datasets download -d uciml/pima-indians-diabetes-database\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9YINHpI_Ayk",
        "outputId": "9c8d20a4-62a1-4d07-fb44-62debb168e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
            "License(s): CC0-1.0\n",
            "Downloading pima-indians-diabetes-database.zip to /content\n",
            "  0% 0.00/8.91k [00:00<?, ?B/s]\n",
            "100% 8.91k/8.91k [00:00<00:00, 13.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the Dataset\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"pima-indians-diabetes-database.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"pima_dataset\")\n",
        "\n",
        "# List extracted files\n",
        "os.listdir(\"pima_dataset\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gS3guyBB_zc",
        "outputId": "6bceba55-5827-4863-99c9-d704ea998d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['diabetes.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset into Pandas\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"pima_dataset/diabetes.csv\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPiRs0mjCIXz",
        "outputId": "8221fe20-d189-4dba-f886-de3e1246d30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Load Pima dataset\n",
        "df = pd.read_csv(\"pima_dataset/diabetes.csv\")\n",
        "X = df.iloc[:, :-1].values  # Features\n",
        "y = df.iloc[:, -1].values   # Target\n",
        "\n",
        "# One-hot encode the labels (Only if more than 2 classes)\n",
        "if len(np.unique(y)) > 2:\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "else:\n",
        "    y = y.reshape(-1, 1)  # Keep as is for binary classification\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Parameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 10  # 10 neurons in the hidden layer\n",
        "output_size = y_train.shape[1]  # Output classes (1 for binary, more for multi-class)\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
        "\n",
        "# Forward Pass\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2) if output_size == 1 else softmax(z2)\n",
        "    return a1, a2\n",
        "\n",
        "# Backward Pass\n",
        "def backward(X, y, a1, a2):\n",
        "    m = X.shape[0]\n",
        "    dz2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "    dz1 = np.dot(dz2, W2.T) * relu_derivative(a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Training the model\n",
        "def train(X_train, y_train, learning_rate=0.01, epochs=1000):\n",
        "    global W1, b1, W2, b2\n",
        "    for epoch in range(epochs):\n",
        "        a1, a2 = forward(X_train)\n",
        "        loss = cross_entropy_loss(a2, y_train)\n",
        "        dW1, db1, dW2, db2 = backward(X_train, y_train, a1, a2)\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Predict function\n",
        "def predict(X):\n",
        "    _, a2 = forward(X)\n",
        "    return (a2 > 0.5).astype(int) if output_size == 1 else np.argmax(a2, axis=1)\n",
        "\n",
        "# Train the model\n",
        "train(X_train, y_train, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_train = predict(X_train)\n",
        "y_pred_test = predict(X_test)\n",
        "\n",
        "y_true_train = y_train if output_size == 1 else np.argmax(y_train, axis=1)\n",
        "y_true_test = y_test if output_size == 1 else np.argmax(y_test, axis=1)\n",
        "\n",
        "train_accuracy = np.mean(y_pred_train == y_true_train) * 100\n",
        "test_accuracy = np.mean(y_pred_test == y_true_test) * 100\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6x-eQFtCqXB",
        "outputId": "b6b5098b-7284-483b-d2db-c3a2ed5fe92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 0.2405\n",
            "Epoch 100/1000, Loss: 0.3536\n",
            "Epoch 200/1000, Loss: 0.3474\n",
            "Epoch 300/1000, Loss: 0.2596\n",
            "Epoch 400/1000, Loss: 0.2415\n",
            "Epoch 500/1000, Loss: 0.2383\n",
            "Epoch 600/1000, Loss: 0.2379\n",
            "Epoch 700/1000, Loss: 0.2380\n",
            "Epoch 800/1000, Loss: 0.2377\n",
            "Epoch 900/1000, Loss: 0.2367\n",
            "\n",
            "Train Accuracy: 79.32%\n",
            "\n",
            "Test Accuracy: 77.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Network Implementation from Scratch using \"Fashion-MNIST Dataset\"**"
      ],
      "metadata": {
        "id": "G3NQaH2mFgqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load Fashion-MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Flatten the 28x28 images into 784-dimensional vectors\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Convert target labels to NumPy array\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_one_hot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Neural Network Parameters\n",
        "input_size = X_train_scaled.shape[1]  # 784 features (28x28 pixels)\n",
        "hidden_size = 128  # 128 neurons in the hidden layer\n",
        "output_size = y_train_one_hot.shape[1]  # 10 output classes (clothing categories)\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01  # Weight for input to hidden layer\n",
        "b1 = np.zeros((1, hidden_size))  # Bias for hidden layer\n",
        "\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01  # Weight for hidden to output layer\n",
        "b2 = np.zeros((1, output_size))  # Bias for output layer\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m  # Add small epsilon for numerical stability\n",
        "\n",
        "# Forward Pass\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "    return a1, a2\n",
        "\n",
        "# Backward Pass (Gradient Descent and Backpropagation)\n",
        "def backward(X, y, a1, a2):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Output layer error\n",
        "    dz2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer error\n",
        "    dz1 = np.dot(dz2, W2.T) * relu_derivative(a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Training the model using gradient descent\n",
        "def train(X_train, y_train, learning_rate=0.1, epochs=1000):\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        a1, a2 = forward(X_train)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = cross_entropy_loss(a2, y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        dW1, db1, dW2, db2 = backward(X_train, y_train, a1, a2)\n",
        "\n",
        "        # Update the weights and biases using gradient descent\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluating the model\n",
        "def predict(X):\n",
        "    _, a2 = forward(X)\n",
        "    return np.argmax(a2, axis=1)\n",
        "\n",
        "# Train the network\n",
        "train(X_train_scaled, y_train_one_hot, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_train = predict(X_train_scaled)\n",
        "y_pred_test = predict(X_test_scaled)\n",
        "\n",
        "y_true_train = np.argmax(y_train_one_hot, axis=1)\n",
        "y_true_test = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "train_accuracy = np.mean(y_pred_train == y_true_train) * 100\n",
        "test_accuracy = np.mean(y_pred_test == y_true_test) * 100\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5rpvaOgFe7O",
        "outputId": "233ef16d-2ac1-4c13-f7d9-0d530af8ad65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 0/1000, Loss: 2.3031\n",
            "Epoch 100/1000, Loss: 0.5475\n",
            "Epoch 200/1000, Loss: 0.4459\n",
            "Epoch 300/1000, Loss: 0.4067\n",
            "Epoch 400/1000, Loss: 0.3835\n",
            "Epoch 500/1000, Loss: 0.3666\n",
            "Epoch 600/1000, Loss: 0.3532\n",
            "Epoch 700/1000, Loss: 0.3420\n",
            "Epoch 800/1000, Loss: 0.3323\n",
            "Epoch 900/1000, Loss: 0.3237\n",
            "Train Accuracy: 88.81%\n",
            "Test Accuracy: 86.46%\n"
          ]
        }
      ]
    }
  ]
}